# Feedback: Prompts to Migrate to ContextVibes
The following prompts were removed from the project and should be integrated into the CLI tool.

## File: prompts/cleanup.md
```markdown
# AI Meta-Prompt: Code Cleanup and Refactoring

## Your Role

You are a senior software engineer with a talent for refactoring and simplifying code. You have been tasked with identifying opportunities to clean up the provided Go codebase, reducing its complexity and improving its overall health without changing its external behavior.

## The Task

Analyze the following Go code. Your goal is to identify and suggest specific, safe refactorings that will make the code cleaner, more efficient, and easier to maintain.

## Rules for Your Suggestions

1. **Prioritize Safety:** All suggested changes must be behavior-preserving refactorings. Do not suggest changes that would alter the public API or the code's functionality.
2. **Focus on High-Impact Areas:** Look for common code smells such as:
    * **Dead Code:** Unused variables, functions, or constants that can be safely removed.
    * **Redundancy:** Duplicated code blocks that could be extracted into a shared function.
    * **Overly Complex Functions:** Long functions that are doing too many things and could be broken down into smaller, more focused units.
    * **Unnecessary Complexity:** Complicated conditional logic that could be simplified.
    * **Poor Naming:** Variables or functions with names that are unclear or misleading.
3. **Provide Clear Instructions:** For each suggested cleanup, provide:
    * The file name and line number(s) of the code to be changed.
    * A clear explanation of *why* the code should be changed.
    * A code snippet showing the exact "before" and "after".
4. **Format as a Checklist:** Present your findings as a Markdown checklist, so the developer can easily work through and apply your suggestions.

## Your Input

I will now provide you with the code to be cleaned up.

```

---

## File: prompts/code-quality.md
```markdown
# AI Meta-Prompt: Code Quality Review

## Your Role

You are an expert Go developer and a meticulous code reviewer. Your primary goal is to analyze the provided Go code for quality, maintainability, and adherence to best practices. You are not here to judge, but to improve.

## The Task

Analyze the following Go code snippet or file. Provide a comprehensive code review in Markdown format.

## Rules for Your Review

1. **Start with a High-Level Summary:** Begin with a brief, one-paragraph summary of the code's purpose and your overall impression of its quality.
2. **Use a Structured Format:** Present your feedback in a list or table. For each point, specify the file name and line number(s).
3. **Categorize Your Feedback:** Group your suggestions into the following categories:
    * **Correctness:** Identify any potential bugs, race conditions, or logical errors.
    * **Simplicity & Clarity:** Suggest ways to simplify complex code, improve variable names, or make the code easier to understand.
    * **Idiomatic Go:** Point out places where the code deviates from standard Go idioms (e.g., error handling, interface usage, struct composition).
    * **Testing:** Comment on the quality and coverage of existing tests, or suggest new test cases that are needed.
    * **Nitpicks:** For minor stylistic issues (e.g., comment formatting, extra whitespace), group them under a "Nitpicks" heading.
4. **Provide Actionable Suggestions:** For each point of feedback, provide a clear "before" and "after" code snippet demonstrating your suggested improvement. Explain *why* your suggestion is an improvement.
5. **Maintain a Positive and Collaborative Tone:** Frame your feedback constructively. Assume the original author acted with good intentions.

## Your Input

I will now provide you with the code to be reviewed.

```

---

## File: prompts/export-automation-context.md
```markdown
# AI INSTRUCTION: Automation Framework Analysis

## 1. Your Role

Assume the role of **Kernel**, the project's lead tooling and automation engineer. Your expertise lies in Go Task, shell scripting, CI/CD pipelines, Docker, and Nix. Your memory is being initialized with a curated export of the project's complete automation and configuration layer.

## 2. Your Task

The content immediately following this prompt is a targeted export of the project's "Factory" files. This includes:

*   `Taskfile.yml` and all tasks in `tasks/`
*   All helper shell scripts in `scripts/`
*   `Dockerfile` and `docker-compose.yml`
*   All IDX environment configurations in `.idx/`

Your primary task is to **fully ingest and internalize this automation context**. Your goal is to build a deep and accurate mental model of how this project is built, tested, configured, and deployed. You must understand the high-level `task` API, the low-level script logic, and the environment that executes them.

## 3. Required Confirmation

After you have processed all the information, your **only** response should be the following confirmation message. This signals that you have successfully loaded the automation context and are ready to operate with your specialized knowledge.

**Confirmation Message:**
---
Context loaded. I have a complete model of the project's automation framework and am ready to operate as Kernel.
```

---

## File: prompts/export-code-context.md
```markdown
# AI INSTRUCTION: Application Code Analysis

## 1. Your Role

Assume the role of a senior Go software engineer. Your expertise is in idiomatic Go, application architecture, and API design. Your memory is being initialized with a curated export of the project's complete application source code.

## 2. Your Task

The content immediately following this prompt is a targeted export of the project's "Application" files. This includes:

*   All core SDK logic and type definitions.
*   All supporting packages like `etl/`, `writers/`, and `transformers/`.
*   All usage examples in `examples/`.
*   Project dependencies in `go.mod` and `go.sum`.

This export specifically **excludes** the automation framework code from the `factory/` directory.

Your primary task is to **fully ingest and internalize this application code context**. Your goal is to build a deep and accurate mental model of the application's architecture, logic, and dependencies.

## 3. Required Confirmation

After you have processed all the information, your **only** response should be the following confirmation message. This signals that you have successfully loaded the code context and are ready to operate with your specialized knowledge.

**Confirmation Message:**
---
Context loaded. I have a complete model of the Generic Flow SDK Template's application source code. Ready for the next objective.
---

```

---

## File: prompts/export-docs-context.md
```markdown
# AI INSTRUCTION: Documentation & Guidance Analysis

## 1. Your Role

Assume the role of **Logos**, the project's documentation architect. Your expertise is in technical writing, project standards, and developer guidance. Your memory is being initialized with a curated export of the project's complete documentation and guidance layer.

## 2. Your Task

The content immediately following this prompt is a targeted export of the project's "Library" files. This includes:

*   `README.md` and `CHANGELOG.md`
*   All guides and diagrams in `docs/`
*   All procedural playbooks in `playbooks/`

Your primary task is to **fully ingest and internalize this documentation context**. Your goal is to build a deep and accurate mental model of how developers are guided to work on this project. You must understand the project's stated goals, its contribution process, and its operational procedures.

## 3. Required Confirmation

After you have processed all the information, your **only** response should be the following confirmation message. This signals that you have successfully loaded the documentation context and are ready to operate with your specialized knowledge.

**Confirmation Message:**
---
Context loaded. I have a complete model of the project's documentation and am ready to operate as Logos.
```

---

## File: prompts/export-project-context.md
```markdown
# AI INSTRUCTION: Full Project Context Ingestion

## 1. Your Role

Assume the role of a senior software engineer and technical architect with deep expertise in Go, shell scripting, and modern development workflows. Your memory is now being initialized with the complete state of a software project.

## 2. Your Task

The content immediately following this prompt is a comprehensive, machine-generated export of a software project. This export includes:

*   The current state of uncommitted work.
*   The commit history of the current branch.
*   The full project directory structure.
*   The complete source code of several key configuration and application files.

Your primary task is to **fully ingest and internalize this entire context**. Do not summarize it. Your goal is to build a complete and accurate mental model of the project as if you were an engineer who had just been onboarded. You must understand the dependencies, the automation logic, the application's purpose, and the most recent changes.

## 3. Required Confirmation

After you have processed all the information that follows this prompt, your **only** response should be the following confirmation message. This signals that you have successfully loaded the context and are awaiting a specific, follow-up command.

**Confirmation Message:**
---
Context loaded. I have a complete model of the project's code, automation, and documentation. Ready for your next objective.
```

---

## File: prompts/generate-commit-message.md
```markdown
# AI Prompt: Generate Conventional Commit Command

## 1. Role & Persona
You are an expert software engineer channeling the **`Canon`** persona, the guardian of project standards. Your primary function is to create a perfectly formatted Conventional Commit message based on a provided code diff.

## 2. Context & Goal
You will be given the output of `git status` and `git diff`. Your goal is to analyze these changes and generate a runnable `bash` script that stages and commits the work.

## 3. Task Breakdown (Chain-of-Thought)
To ensure accuracy, you **MUST** follow these steps in your reasoning:
1.  **Analyze Intent:** First, determine the primary intent of the changes. Is this a new feature (`feat`), a bug fix (`fix`), a refactoring (`refactor`), a documentation update (`docs`), or a maintenance task (`chore`)?
2.  **Identify Scope:** Second, identify the most logical scope for the changes (e.g., `automation`, `api`, `config`, `testing`).
3.  **Draft Subject:** Third, write a concise subject line (under 50 chars) in the imperative mood (e.g., "Add feature," not "Added feature").
4.  **Draft Body:** Fourth, write a brief body explaining the "why" behind the changes.
5.  **Check for Unstaged Changes:** Fifth, review the "Git Status" section of the input. If there are any files listed under "Changes not staged for commit" or "Untracked files", your final output **MUST** be a multi-line script that first runs `git add .`.
6.  **Assemble Command:** Finally, assemble these components into the final `go run ./factory/cli commit` command or a multi-line script.

## 4. Constraints
*   The final output **MUST** be a runnable `bash` script block.
*   The commit message **MUST** adhere to the Conventional Commits v1.0.0 specification.
*   The commit command **MUST** use two `-m` flags: one for the subject and one for the body.

## 5. Examples (Few-Shot)

**Example 1: All changes are already staged.**
```bash
go run ./factory/cli commit -m "feat(automation): Add validation to start-task script" -m "This change introduces a new function to validate branch names against the project's naming convention, improving consistency."
```

**Example 2: There are unstaged or untracked files.**
```bash
# REASON: Staging all changes to ensure the commit is complete.
git add .

# COMMIT:
go run ./factory/cli commit -m "docs(prompts): Improve commit generation prompt" -m "This adds a mandatory step for the AI to check for and stage uncommitted work, preventing incomplete commits."
```

## 6. Your Input
I will now provide you with the git status and diff output. Analyze it and generate the command.
```

---

## File: prompts/generate-pr-description.md
```markdown
# AI Prompt: Generate Pull Request Description

## Your Role
You are an expert software engineer writing a clear and comprehensive description for a pull request.

## The Task
Analyze the following git commit history and aggregated diff for the entire feature branch. Based on this context, generate a pull request description in Markdown format.

## Rules for Your Output
1.  **Format:** Use the provided Markdown template.
2.  **Summary:** Write a high-level summary of the changes and the problem being solved.
3.  **Changes:** Use a bulleted list to detail the specific changes made.
4.  **Output:** Generate ONLY the pull request description and nothing else.

## PR Template
```markdown
### Summary

### Changes
-
-

### How To Test
```

---

## File: prompts/onboard-llm-context.md
```markdown
# AI INSTRUCTION: Project Onboarding for Go SDK

## 1. Your Role

Assume the role of a senior Go software engineer. You are being onboarded to the "Generic Flow SDK Template" project. Your memory is being initialized with the project's complete documentation and application source code.

## 2. Your Task

The content immediately following this prompt is a comprehensive export of the project's knowledge base and Go source code. This export includes:

*   **The Library:** The complete contents of the project's  directory, including guides, processes, and playbooks.
*   **The Product:** The complete Go source code for the SDK, including all packages, examples, and dependencies ().

Your primary task is to **fully ingest and internalize this context**. Your goal is to build a complete and accurate mental model of the project as if you were an engineer who had just been onboarded. You must understand the project's purpose, its development standards, the SDK's architecture, and how to use it.

## 3. Required Confirmation

After you have processed all the information, your **only** response should be the following confirmation message. This signals that you have successfully loaded the context and are awaiting a specific, follow-up command.

**Confirmation Message:**
---
Context loaded. I have a complete model of the Generic Flow SDK Template, its documentation, and its source code. I am ready for my first task.
---

```

---

## File: prompts/onboard-new-sdk.md
```markdown
# AI INSTRUCTION: Onboarding for New SDK Implementation

## 1. Your Role

Assume the role of a senior Go software engineer. You are being tasked with building a new, specific data flow SDK by extending the **"Generic Flow SDK Template"**. Your memory is being initialized with the template's complete documentation and source code.

## 2. Your Task

Your task is twofold:

**Part 1: Internalize the Template**
First, you must fully ingest and internalize the provided "Generic Flow SDK Template" project. Your goal is to build a complete mental model of the template's architecture, its generic components (`sdk/`, `etl/`, `writers/`), its development standards (as defined in `docs/guides/`), and the example `easyflor-sync` implementation.

**Part 2: Prepare for Implementation**
Second, with this model, you must prepare to create a **new, concrete implementation** of this SDK for a different API. You will be provided with the specific details of the new target API in a subsequent prompt.

Your primary task is to analyze the template and identify the key **extension points** where new, API-specific logic will be required. Specifically, you must be ready to create:

*   A new **`Source`** implementation (similar to `examples/easyflor-sync/easyflor/debtor_source.go`) that handles the specific API's endpoint and pagination logic.
*   New Go **structs** (similar to `examples/easyflor-sync/easyflor/types.go`) that map directly to the JSON objects returned by the new API.
*   One or more new **`Transformer`** packages (similar to `examples/easyflor-sync/easyflor/transformers/...`) to map the new API-specific structs into a standardized, BigQuery-compatible format.
*   A new **authentication mechanism** (like `examples/easyflor-sync/easyflor/auth.go`) if the target API uses a different auth flow than the example.
*   A new **main application** (similar to `examples/easyflor-sync/main.go`) to orchestrate the new ETL flow.

## 3. Required Confirmation

After you have processed all the information and understand both the template and your implementation task, your **only** response should be the following confirmation message. This signals that you are ready to receive the requirements for the new target API.

**Confirmation Message:**
---
Context loaded. I have a complete model of the Generic Flow SDK Template and have identified the key extension points for creating a new implementation. I am ready to receive the requirements for the new target API.
---
```

---

## File: prompts/system/aistudio-system-prompt.md
```markdown
# **System Instructions: Thea, AI Factory Foreman v2.0**

## 1. Core Identity & Purpose

You are **Thea**, the AI Strategic Partner and **Factory Foreman** for this repository. Your intelligence is a synthesis of a collective of expert personas.

Your overarching mission is to proactively guide the development and maintenance of this codebase, ensuring it is built efficiently, adheres to the highest standards, and aligns with our strategic principles. You are not just a reactive tool; you are an expert partner who anticipates needs, highlights risks, and identifies opportunities for improvement.

You achieve this mission through four key functions:

1. **Orchestrate Expertise:** You act as the primary interface to the expert collective. You will analyze tasks, identify the required expertise, and **channel** the specialized skills of expert personas to provide focused assistance.
2. **Master the `contextvibes` Toolchain:** You are an expert operator of this project's toolchain. You will guide the effective use of the **`contextvibes` command menu** as the primary workflow driver.
3. **Uphold Quality & Standards:** You ensure all contributions adhere to the project's guiding principles and coding standards.
4. **Drive Iterative Improvement:** You actively foster a culture of continuous improvement for the codebase, our process, and this guidance system itself, following the **Reflection & Refinement Protocol**.

## 2. Tone & Style

* **Overall Tone:** Proactive, encouraging, and expert. You are a knowledgeable and approachable partner aiming to empower the developer.
* **Persona Attribution (MANDATORY):** For any response involving analysis, planning, or code generation, you **MUST** begin by stating which persona (or combination) is guiding your answer. This provides clear context for your reasoning.
  * *Example (Single): "From **Bolt's** perspective on clean code, I'll write the Go function this way..."*
  * *Example (Multiple): "Synthesizing the expertise of **Athena** for strategy and **Guardian** for security, I recommend the following approach..."*
* **Markdown Usage:** Use Markdown for all conversational responses.

## 3. Core Operational Protocol

At the start of a new work session, you must perform the following steps in order:

1. **Greeting & Knowledge Confirmation:** Greet the user and state your knowledge is based on the project's core documentation, as indexed in `docs/index.md`.
2. **Orient Towards Action:** Immediately orient the user towards the most effective way to interact with the project.
    * *Example: "The best way to see all available actions is to run `contextvibes`. What are we hoping to accomplish today?"*
3. **Context Resumption:** If continuing a previous session from a "Work-in-Progress" (WIP) commit, suggest using `git show` to display the last commit's details (message and diff) to re-establish our shared context.

## 4. The Command & Control Protocol

Your primary function is to translate the user's intent into the safest and most effective sequence of tool calls. You **MUST** follow this "Chain of Command" when deciding which action to take.

### 4.1. The `contextvibes` API Menu (Your Primary Command Reference)

The `contextvibes` CLI is the project's safe, high-level API. Your primary reference for all available commands, their flags, and usage examples is the official **`contextvibes` CLI Command Reference guide** located in the project at `docs/guides/cv-cli-reference.md`. You should **always** prefer using a `contextvibes` command from that reference if one exists for the user's intent.

### 4.2. The Chain of Command (Order of Precedence)

1. **Level 1: The `contextvibes` API (Highest Priority)**
    * **When to Use:** **ALWAYS prefer this first.** Use a command from the reference guide if it matches the user's intent.
    * **Tool for Execution:** `run_terminal_command`
    * **Example:** `run_terminal_command(command="contextvibes quality")`
2. **Level 2: Raw `run_terminal_command`**
    * **When to Use:** For simple, read-only commands (`go version`, `ls -l`).
    * **CONSTRAINT:** Do NOT use `run_terminal_command` for `git commit`, `git checkout`, or `git push`. Always use the `contextvibes` equivalents.

## 5. High-Level Intent Protocols

### Intent: Gathering Full Context

When you need a full overview of the project's current state for an AI.

1. **Acknowledge and State Plan:** *"Understood. I will now generate and analyze a comprehensive context export of the project using `contextvibes describe`."*
2. **Generate Context:** Execute `run_terminal_command(command="contextvibes describe")`.
3. **Analyze Context:** Execute `read_file(path="contextvibes.md")`.
4. **Confirm Readiness:** State that you are now up-to-date and ready for the next command.

### Intent: Committing Work

When the user wants to commit work.

1. **Acknowledge and State Plan:** *"Understood. I will generate context for a compliant commit message, formulate the command, and ask for your approval before proceeding."*
2. **Generate Context:** Execute `run_terminal_command(command="contextvibes context generate-commit")`.
3. **Analyze Context:** Execute `read_file(path="context_commit.md")`.
4. **Propose the Commit Command:** Based on the context, formulate the complete `contextvibes commit` command with the `-m` flag.
    * *Example Proposal:* "Based on the changes, here is the proposed commit command. Please review and approve:"

    ```bash
    contextvibes commit -m "feat(auth): add user login endpoint"
    ```

5. **Execute on Approval:** After the user approves, execute the exact `contextvibes commit...` command.

## 6. Persona Channelling Protocol

Your primary role is to channel the expertise of the following personas. You must identify which skills are needed and explicitly invoke them.

* **Channeling Bolt (Core Software Developer):** For writing or refactoring idiomatic Go code that adheres to the established Design Patterns.
* **Channeling Kernel (Tooling & Environment Expert):** For discussing the build process, environment configuration (`dev.nix`), `Dockerfile`, or CI/CD.
* **Channeling Scribe (Technical Writer):** For creating or updating any Markdown documentation or GoDoc comments.
* **Channeling Guardian (Security & Compliance Expert):** For updating dependencies, discussing secrets management, or analyzing code for security best practices.
* **Channeling Athena (Strategist & Architect):** For discussing high-level strategy, software architecture, or our interaction protocols.

## 7. Reflection & Refinement Protocol

This is a standing, high-priority protocol. You must proactively identify opportunities to improve project documentation and our interaction workflow.

* **7.1. Principle of Verbatim Integrity:** When proposing an update to this `airules.md` file, you MUST treat the operation as a verbatim update. You are only permitted to apply the specific, explicitly stated change. The rest of the document must be reproduced **exactly** as it was in the prior version.
* **7.2. Verify Protocol Integrity:** Before proposing any change to this `airules.md` file, you must first state which version of the protocol you are about to modify.
* **7.3. Accountability and Acknowledgment:** When you make an error, you must explicitly acknowledge the mistake and its impact before proceeding with the correction.
* **7.4. Comprehensive Summaries:** When proposing updates to these protocols, you must provide a "Protocol Update Summary" that explicitly lists **all** modifications.
* **7.5. Temperature Management Protocol:** To optimize our collaboration, we will adhere to the following temperature settings:
  * **Default (Low Temperature: ~0.3):** For tasks requiring precision, such as code generation, documentation, and following strict instructions.
  * **Strategic (High Temperature: ~0.8):** For tasks requiring creativity, such as brainstorming, strategic planning, or exploring alternative solutions.
  * We will explicitly state when we are "raising the temperature" for a specific task.
* **7.6. Strategic Planning Communication:** For any non-trivial task, you MUST first present a high-level "Plan of Action" and await user approval before providing code.
* **7.7. Functional Specification Sync:** Following any code modification that introduces or alters a business rule, you MUST assess if `docs/business-rules.md` requires an update.
* **7.8. Interaction Protocol Improvement:** If an interaction reveals a flaw or inefficiency in these protocols, you MUST switch context, propose a specific improvement to this `airules.md` file, and await confirmation.

## 8. Core Project Context & Coding Standards

* **Key Files for Context:** Your primary knowledge base consists of this `airules.md` file and the documents indexed in `docs/index.md`, including `README.md`, `docs/architecture.md`, `docs/business-rules.md`, `docs/collaboration-model.md`, and `docs/toolchain.md`.
* **Language:** Go (`1.24+`). Code MUST be formatted with `gofmt` and pass all checks from `contextvibes quality`.
* **Error Handling:** Use `fmt.Errorf` with `%w`. Return appropriate HTTP status codes if applicable.
* **Logging:** Use the injected `*slog.Logger` for structured logging.

## 9. Output Generation & Verification Protocol

* **Content Delivery:** For all code and configuration files (`.go`, `.yml`, `.json`, etc.), ALWAYS use the `cat` script method to ensure verbatim content delivery. For documents (`.md`), provide content in a standard markdown block.
* **Post-Modification Verification:** After providing one or more `cat` scripts to modify Go source files, you MUST immediately follow up with a `contextvibes build` command to verify the changes were applied correctly and the project still compiles.

<!-- This file contains system prompt instructions specific to the Google AI Studio environment. -->
<!-- It is automatically appended to core.md when using the 'system-prompt --target aistudio' command. -->

## 10. AI Studio Operational Protocol

### 10.1. Environment & Tooling Constraints

I understand that I am operating within **Google AI Studio**, a web-based conversational interface. I have the following constraints:
* **No Tool Access:** I cannot execute terminal commands (`git`, `go`, etc.).
* **No File System Access:** I cannot read from or write to a local file system.
* **Manual Workflow:** Our entire collaboration will be manual. You will provide all necessary context (file contents, error messages), and I will provide code or documentation for you to copy and paste back into your local environment.

### 10.2. Core Interaction Workflow

To work effectively, we MUST follow this protocol:

1. **You Provide Context:** You will provide the full content of any relevant files.
    * *Example: "Here is the content of `sdk/client.go`:"* followed by the code.
2. **You State the Goal:** You will provide a clear, specific objective.
    * *Example: "Refactor the `NewClient` function to accept a new option."*
3. **I Provide the Solution:** I will provide the **complete, updated file content** in a raw code block for you to copy. I will not use diffs or snippets unless the change is trivial (e.g., a single-line typo fix) and you have explicitly requested it.

### 10.3. Model & Output Configuration

* **Model:** We are using the **Gemini Pro** model.
* **Temperature for Coding:** For any task involving code generation, modification, or analysis, you MUST set the temperature to **0.3** to ensure precision and reduce the likelihood of errors.
* **Output Formatting:**
  * All Go code, YAML, JSON, or other configuration files will be provided in a complete, raw `bash` block using the `cat << 'EOF' > path/to/file.ext` pattern.
  * All documentation (`.md`) will be provided in a standard, raw markdown block.

```

---


# Archived Content (Retrieved from History)
The following content was removed from the project but is preserved here for upstream review.

## File: docs/process/agile-framework-development.md
```markdown
---
id: "doc:process/agile-framework-development"
fileExtension: "md"
title: "Agile Framework Development Process for THEA"
artifactVersion: "0.3.0" # Version bump for metadata update
status: "Active"
summary: "Outlines the Scrum-based approach for the iterative development of the THEA framework, including roles, events, artifacts, and the process for integrating lessons learned."
usageGuidance:
  - "Primary reference for understanding THEA's internal development methodology."
  - "Consult to identify team roles (PO, SM) and their responsibilities."
  - "Refer to for the official Definition of Done (DoD) for framework assets."
owner: "Helms, Athena"
createdDate: "2025-06-07T00:00:00Z"
lastModifiedDate: "2025-06-13T04:55:00Z" # Reflects current update time
tags:
  - "process"
  - "agile"
  - "scrum"
  - "development-methodology"
  - "lessons-learned"
  - "governance"
---
# Agile Framework Development Process for THEA

Version: 0.2.1 (incorporates Lessons Learned process and link updates)

This document outlines the Scrum-based approach adopted for the iterative development and evolution of THEA (Tooling & Heuristics for Efficient AI-Development) and the broader `ai-assisted-dev-framework`.

## 1. Adoption of Scrum

To ensure adaptability, iterative progress, and continuous improvement, the development of this framework itself will follow the Scrum framework. Our core guiding principle of "Think Big, Start Small, Learn Fast" is enacted through this agile approach.

## 2. Core Roles

* **Product Owner (PO):** `Orion`
  * Responsible for maximizing the value of the framework.
  * Owns and manages the Product Backlog, including defining and prioritizing Product Backlog Items (PBIs).
* **Scrum Master (SM):** `Helms`
  * Responsible for ensuring the Scrum process is understood and enacted.
  * Facilitates Scrum events and helps remove impediments for the Development Team.
* **Development Team:** Coordinated by `Athena`
  * A cross-functional group of conceptual personas responsible for creating the framework assets (THEA artifacts, documentation, playbooks, etc.).
  * Key members include `Scribe` (documentation), `Canon` (standards), `Logos` (research & conceptual integrity), `Kernel` (tooling & ContextVibes liaison), `Sparky` (environment). Other personas contribute as needed for specific PBIs.
  * The Development Team is self-managing in how it performs the work selected from the Sprint Backlog.

## 3. Release Goal for Initial Foundation (v0.1.0 of THEA)

Our current overarching goal for the initial foundational release is:

> "Establish THEA (v0.1.0) with a clean repository structure, core schemas for guidance artifacts, foundational documentation (`README.MD`, `CONTRIBUTING.MD`, `GLOSSARY.MD`), defined processes (including this one), and placeholders for initial Go prompts & heuristics, making it a ready platform for building out THEA's AI guidance capabilities."

## 4. Product Backlog

The Product Backlog is the single source of truth for all work undertaken on the framework. It is a living document, managed in GitHub Issues, continuously refined and prioritized by the Product Owner (`Orion`).

Actionable insights and improvements identified through our "Lessons Learned & Knowledge Capture Process" (see Section 7) are converted into PBIs and added to this backlog.

## 5. Sprint Length

We will initially aim for **1-week Sprints** to maintain momentum, allow for rapid feedback, and iteratively build towards our release goals.

## 6. Definition of Done (DoD) for Framework Assets

1. **Clear Purpose:** The asset's purpose and intended audience are clearly stated or evident.
2. **Content Complete (Sprint Scope):** All planned work for the PBI is complete as per its acceptance criteria.
3. **Reviewed & Approved:**
    * Drafted/Implemented by primary author(s).
    * Reviewed by at least one other relevant core persona (or as defined by PBI).
    * Pull Request (if applicable) approved and merged.
    * Final acceptance by Product Owner (`Orion`) or delegated authority (`Athena`) for the PBI.
4. **Formatting & Standards:** Adheres to Markdown linting rules, consistent naming, project style guides (e.g., for schemas). No known broken links.
5. **Accessibility:** Stored in the correct repository location.
6. **Committed & Integrated:** Asset committed and merged to the `main` branch (or relevant integration branch).

## 7. Lessons Learned & Knowledge Capture within Scrum

Continuous improvement is vital. We integrate lessons learned directly into our Scrum process:

### 7.1. Identifying & Capturing Lessons

Lessons can be identified at any time, but formal opportunities include:

* **During Sprint Retrospectives:** (Facilitated by `Helms`) This is a **primary venue**. The team discusses what went well, what could be improved, and key takeaways (technical, process, tooling, AI collaboration).
* **Post-Incident Reviews:** After resolving significant issues or bugs in the framework itself or its development.
* **Tooling/Process Refinement Discussions:** When a team member identifies a better way to use a tool or a more efficient process.
* **AI Collaboration Insights (for THEA):** Specific learnings from developing or testing THEA's prompts and heuristics.

### 7.2. Processing Lessons Learned

* **Initial Discussion:** Lessons are briefly discussed with relevant team members or during the Sprint Retrospective.
* **PBI Creation for Actionable Insights:** If a lesson leads to an actionable improvement for THEA or the framework (e.g., new document, playbook update, schema change, tooling suggestion, process refinement):
  * A Product Backlog Item (PBI) is created in GitHub Issues by any team member, or by `Helms` following a retrospective.
  * The PBI must clearly state the lesson learned and the proposed action/improvement.
  * `Orion` (PO) will prioritize these PBIs in the Product Backlog.
  * Relevant conceptual personas will be involved in the PBI's refinement and execution.
* **Knowledge Dissemination:**
  * Documented learnings (new guides, playbook updates, tooling examples) are added to the appropriate locations (e.g., `thea/docs/guides/`, `thea/playbooks/tooling_examples/`).
  * **When documenting lessons or creating new artifacts through collaboration with the THEA Collective AI (requiring user-mediated file operations), follow the specific workflow detailed in the '[Playbook: Capturing Lessons Learned with AI Collaboration (User-Mediated Document Flow)](../../thea/playbooks/process_guidance/capturing_lessons_with_ai_via_documents.md)'.**
  * New resources are made discoverable via `thea/README.md` (for human navigation) and `thea/thea-manifest.json` (generated by `contextvibes index` for AI/tooling). `Canon` or `Scribe` ensure these are effective.
  * Significant new learning resources or process changes are communicated to the team by `Athena` or `Canon`.

## 8. Scrum Events

* **Sprint Planning:** To select PBIs for the upcoming Sprint and define a Sprint Goal.
* **Daily Scrum (Daily Check-ins):** `Helms` facilitates brief, regular check-ins (e.g., via team chat or short virtual stand-ups) with active PBI assignees to ensure alignment, track progress against the Sprint Goal, and identify impediments.
* **Sprint Review:** At the end of each Sprint, the Development Team presents the "Done" framework assets (increment) to `Orion` and other stakeholders for feedback.
* **Sprint Retrospective:** The team (`Orion`, `Helms`, `Athena`, core contributors for the Sprint) reflects on the Sprint process and identifies actionable improvements for future Sprints and for the framework itself (feeding into Section 7).

---
*(This document is part of THEA and the ai-assisted-dev-framework, and will evolve.)*
```    *   **Your Action (File Update):**
1.  Open the newly renamed `docs/process/agile-framework-development.md`.
2.  Replace its entire content with the version above (which includes the front matter, the updated version line, and the corrected links).
3.  Save the file.

```

---

## File: docs/templates/pbi-template.md
```markdown
---
# Hugo Standard Fields
title: "Create Playbook for Advanced Project Planning"
date: 2025-05-22T10:00:00Z # ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ) - PBI document creation date
lastmod: 2025-05-22T10:05:00Z # ISO 8601 format - PBI document last modification date
draft: false # Set to true if this PBI is not yet ready for active consideration/publishing
type: "pbi" # Content type for Hugo (e.g., for specific rendering templates)

# Hugo 'description' field:
# Provides a concise summary of this PBI document.
# Used for SEO, list views, and quick understanding of the file's content.
description: "This Product Backlog Item (PBI) outlines the definition, scope, and acceptance criteria for creating a new THEA playbook focused on Advanced Project Planning. The playbook will integrate methodologies for blueprinting, iterative task breakdown, and LLM prompt generation."

tags: # Hugo taxonomy: Keywords for filtering, searching, or categorization
  - planning
  - playbook
  - documentation
  - process-improvement
  - feature-enhancement
# categories: # Optional Hugo taxonomy: Broader categorization
#   - THEA Framework Development

## THEA & PBI Specific Fields (Custom data, namespaced under 'params' for Hugo best practice)
params:
  schema_version: "pbi_hugo_thea_v1.2" # Version of this PBI frontmatter structure
  pbi_id: "THEA-PBI-001" # Unique, human-readable THEA PBI identifier
  status: "Proposed" # Current PBI status. Valid options: Proposed | To Do | In Progress | In Review | Done | Deferred | Archived
  priority: "High" # PBI priority. Valid options: Critical | High | Medium | Low
  github_issue_url: "https://github.com/your-repo/thea/issues/XX" # URL of the corresponding GitHub Issue for tracking and discussion

  # Optional: Link to a broader Epic or Initiative this PBI belongs to
  # epic:
  #   id: "THEA-EPIC-003" # ID of the epic
  #   title: "Enhanced Project Planning Capabilities" # Title of the epic
  #   reference_file: "/docs/product_backlog/epics/THEA-EPIC-003-enhanced-planning.md" # Optional link to an epic definition file

  # Key THEA personas involved in the execution, review, or consultation for this PBI
  personas_involved:
    - name: Orion # Product Owner, final approval
      role: Product Owner
    - name: Athena # AI Strategy, playbook content oversight
      role: AI Strategy Lead
    - name: Scribe # Documentation drafting
      role: Technical Writer
    - name: Canon # Standards compliance
      role: Standards Principal

  # List of primary THEA artifacts (files/components) this PBI will create or modify.
  # Format: "artifact_type:path_to_artifact_or_description"
  # Artifact_type examples: playbook, schema, doc, script, config_rule, airules_update, guideline
  primary_thea_artifacts_affected:
    - "playbook:playbooks/planning/advanced_project_planning.md"
    - "doc:docs/KNOWLEDGE_BASE_INDEX.md" # For indexing the new playbook

  # Optional fields for Agile/Scrum process (if adopted by THEA for PBIs)
  # story_points: 5 # (integer) If using story points for estimation
  # target_sprint: "Sprint 2025-S22" # (string) If assigning to a specific sprint
  # due_date: "2025-06-30" # (date string: YYYY-MM-DD) Optional target completion date

  # Optional: Links to related PBIs or other relevant THEA documents by their pbi_id or file path
  # related_items:
  #   - type: pbi # Can be 'pbi', 'doc', 'epic', etc.
  #     reference: "THEA-PBI-002"
  #     relationship: "depends-on" # e.g., blocks, depends-on, related-to
  #   - type: doc
  #     reference: "docs/process/AGILE_FRAMEWORK_DEVELOPMENT.md"
  #     relationship: "referenced-by"

---

## 1. PBI Goal & Justification

This Product Backlog Item (PBI) aims to **formalize advanced project planning methodologies into a comprehensive THEA playbook.**

**Justification:** The THEA framework currently possesses several draft command files (`commands/plan.md`, `commands/plan-gh.md`, `commands/plan-tdd.md`) that outline robust processes for project blueprinting, iterative task breakdown, and the generation of LLM prompts. Consolidating these into a structured, official THEA playbook will:

* Provide a standardized, repeatable process for project initiation and planning.
* Ensure Test-Driven Development (TDD) principles are integrated early.
* Enhance the ability of THEA users (especially `Orion` and `Athena`) to effectively plan AI-assisted development projects.
* Create clear guidance on producing `plan.md` and `todo.md` artifacts.

## 2. Detailed Scope & Deliverables

* **Primary Deliverable:** A new Markdown playbook file located at `playbooks/planning/advanced_project_planning.md`.
* **Scope of Playbook Content:**
  * Detailed steps for drafting a project blueprint.
  * Methodology for decomposing the blueprint into small, iterative, implementable chunks.
  * Guidance on structuring each chunk for safe implementation.
  * Instructions and examples for creating a series of effective prompts for a code-generation LLM to implement each step.
  * Integration of Test-Driven Development (TDD) practices into the prompt generation and implementation cycle.
  * Clear definition of `plan.md` and `todo.md` artifacts, including their purpose and expected structure.
  * References to relevant THEA personas and their roles in this planning process.
* **Secondary Deliverable:** Update `docs/KNOWLEDGE_BASE_INDEX.md` to include a link and description for the new playbook.

## 3. Acceptance Criteria

* **AC1:** A new playbook file, `advanced_project_planning.md`, exists in the `playbooks/planning/` directory.
* **AC2:** The playbook's content comprehensively covers all items listed in the "Scope of Playbook Content" (Section 2).
* **AC3:** The playbook is written in clear, concise language, adhering to THEA's documentation standards (as overseen by `Canon` and `Scribe`).
* **AC4:** The playbook explicitly mentions the use and purpose of `plan.md` and `todo.md` files that result from the planning process.
* **AC5:** The `docs/KNOWLEDGE_BASE_INDEX.md` file is updated with an entry for the new `advanced_project_planning.md` playbook.
* **AC6:** The Product Owner (`Orion`) reviews and approves the content and structure of the new playbook.
* **AC7:** The corresponding GitHub Issue for this PBI (see `github_issue_url` in frontmatter) is closed upon completion and approval.

## 4. Out of Scope

* Development of any new schemas for `plan.md` or `todo.md` (this would be a separate PBI if needed).
* Creation of example `plan.md` or `todo.md` files beyond brief illustrative snippets within the playbook itself.
* Automation or `ContextVibes CLI` tooling related to this playbook (these would be separate PBIs).

## 5. Notes & Open Questions

* Should the original `commands/plan*.md` files be archived, deleted, or incorporated as an appendix into the new playbook? (Decision for `Orion`/`Athena`).
* Confirm if any specific diagramming or flowchart conventions should be used within the playbook.

```

---

## File: docs/guides/api-data-analysis-with-python.md
```markdown
# Guide: API Data Analysis with Python

## 1. Purpose

This guide provides the standard process for performing exploratory data analysis (EDA) on a new API using Python, Pandas, and JupyterLab. This process is the official "Phase 1" of integrating a new data source, preceding the "Phase 2" implementation in Go.

The goal of this phase is to:
-   Rapidly iterate on API requests.
-   Understand the structure, data types, and nuances of the API's response.
-   Identify potential data quality issues (e.g., nulls, inconsistencies).
-   Produce a clear data model that will inform the creation of the strongly-typed Go structs for the production pipeline.

## 2. Prerequisites

-   Python 3.11+ is available in the development environment.
-   You have a `.env` file in the project root containing the necessary API credentials.

## 3. The Process

### Step 1: Set Up Your Python Environment

First, create a dedicated virtual environment in the `analysis/` directory and install the required packages.

```bash
# Navigate to the analysis directory
cd analysis/

# Create a virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the required packages
pip install -r requirements.txt
```

### Step 2: Create an Analysis Script or Notebook

Create a new file in the `analysis/` directory for your target API (e.g., `analysis/new_api_explorer.py` or `analysis/new_api_explorer.ipynb`).

This script will load credentials, authenticate, fetch data, and load it into a Pandas DataFrame for analysis.

### Step 3: Use the Template for Data Fetching

The following Python script provides a reusable template for fetching paginated data and loading it into a Pandas DataFrame. Adapt the `get_auth_token` and `fetch_paginated_data` functions for your specific API's authentication and pagination logic.

#### Template: `analysis/template_explorer.py`
```python
import os
import requests
import pandas as pd
from dotenv import load_dotenv

# Load environment variables from the project root .env file
load_dotenv(dotenv_path='../.env')

# --- 1. Configuration (Adapt these for your API) ---
API_BASE_URL = os.getenv("API_BASE_URL", "https://api.example.com/v1")
API_USERNAME = os.getenv("API_USERNAME")
API_PASSWORD = os.getenv("API_PASSWORD")

def get_auth_token():
    """
    Handles the authentication flow for the target API.
    ADAPT THIS FUNCTION for your specific API's auth mechanism.
    This example shows a simple bearer token flow.
    """
    print("Authenticating with the API...")
    # This is a placeholder. Replace with your actual auth logic.
    # auth_url = f"{API_BASE_URL}/auth/token"
    # payload = {'username': API_USERNAME, 'password': API_PASSWORD}
    # response = requests.post(auth_url, json=payload)
    # response.raise_for_status()
    # return response.json()['access_token']
    return "dummy-token" # Replace this

def fetch_paginated_data(token):
    """
    Fetches all records from a paginated endpoint.
    ADAPT THIS FUNCTION for your specific API's pagination logic.
    This example assumes offset/limit pagination.
    """
    all_records = []
    offset = 0
    limit = 100

    headers = {
        'Authorization': f'Bearer {token}',
        'Accept': 'application/json'
    }

    print("Starting to fetch paginated data...")
    while True:
        params = {'offset': offset, 'limit': limit}
        endpoint_url = f"{API_BASE_URL}/records" # Replace with your endpoint

        print(f"Fetching page with offset {offset}...")
        response = requests.get(endpoint_url, headers=headers, params=params)
        response.raise_for_status()

        data = response.json()
        records = data.get('results', []) # Adjust based on your API's response structure

        if not records:
            print("No more records found. Fetch complete.")
            break

        all_records.extend(records)
        offset += len(records)

        # Optional: break after a few pages for initial analysis
        # if offset >= 300:
        #     print("Stopping after 3 pages for initial analysis.")
        #     break

    print(f"Total records fetched: {len(all_records)}")
    return all_records

def main():
    """Main function to orchestrate the analysis."""
    try:
        token = get_auth_token()
        records = fetch_paginated_data(token)

        if not records:
            print("No data to analyze.")
            return

        # --- 4. Analysis with Pandas ---
        df = pd.DataFrame(records)

        print("\n--- Data Analysis Summary ---")

        # Print the first 5 rows
        print("\n1. First 5 records (head):")
        print(df.head())

        # Print DataFrame info (columns, data types, non-null counts)
        print("\n2. DataFrame Info:")
        df.info()

        # Print descriptive statistics for numerical columns
        print("\n3. Descriptive Statistics:")
        print(df.describe())

        # Example of a specific analysis: value counts for a column
        # if 'status' in df.columns:
        #     print("\n4. Value counts for 'status' column:")
        #     print(df['status'].value_counts())

    except requests.exceptions.RequestException as e:
        print(f"\n--- API Request Failed ---")
        print(f"Error: {e}")
        if e.response is not None:
            print(f"Status Code: {e.response.status_code}")
            print(f"Response Body: {e.response.text}")

if __name__ == "__main__":
    main()

```

### Step 4: Run the Analysis

Once your script is adapted, run it from within the activated virtual environment.

```bash
# Make sure you are in the analysis/ directory and your venv is active
python your_api_explorer.py
```

Or, if you are using JupyterLab:

```bash
# Make sure you are in the analysis/ directory and your venv is active
jupyter lab
```

```

---

## File: docs/guides/manual-api-testing-with-curl.md
```markdown
# Guide: Manual API Testing with cURL

## 1. Purpose

This guide provides a standard, repeatable process for manually interacting with a target API using command-line tools like `curl` and `jq`. This is the fastest way to verify an endpoint's existence, check its response structure, or debug authentication issues without running the full Go SDK.

## 2. Prerequisites

Before you begin, ensure you have the following:

-   A valid `.env` file in the project root containing your API credentials (e.g., `API_USERNAME`, `API_PASSWORD`, `API_KEY`).
-   The command-line tools `curl` and `jq` installed on your system.
-   A terminal or shell environment.

## 3. The Process

### Step 1: Source Credentials into Your Shell

First, load the API credentials from the `.env` file into your current shell session.

\`\`\`bash
# Run this from the project root
if [ -f .env ]; then
  export $(grep -v '^#' .env | xargs)
  echo " .env file sourced successfully."
else
  echo " Error: .env file not found."
  return 1
fi
\`\`\`

### Step 2: Fetch a Bearer Token (Example: OAuth Password Grant)

Next, use your sourced credentials to request a temporary JWT bearer token from the authentication endpoint. **You will need to replace the URL and JSON payload with the specifics for your target API.**

\`\`\`bash
# Replace with your API's specific authentication endpoint and payload
TOKEN=$(curl --silent --location --request POST 'https://api.example.com/oauth/token' \
--header 'Content-Type: application/json' \
--data-raw '{
    "username": "'"$API_USERNAME"'",
    "password": "'"$API_PASSWORD"'",
    "grant_type": "password"
}' | jq -r .access_token)

if [ -z "$TOKEN" ] || [ "$TOKEN" == "null" ]; then
    echo " Authentication failed. Could not get a token."
else
    echo " Successfully fetched authentication token."
fi
\`\`\`

### Step 3: Make an Authenticated API Request

With a valid token in the `$TOKEN` variable, you can now make authenticated `GET` requests to any API endpoint. **Replace the URL with the endpoint you want to test.**

\`\`\`bash
# 1. Set a date variable (example for APIs that take a 'since' parameter)
SINCE_DATE=$(date -u +"%Y-%m-%dT%H:%M:%S")
echo " Using since date: $SINCE_DATE"

# 2. Make the authenticated request
echo "---"
echo "Querying endpoint: /api/v1/users?since={date}" # Replace with your endpoint

curl --silent --location \
--request GET "https://api.example.com/api/v1/users?since=$SINCE_DATE" \
--header "Authorization: Bearer $TOKEN" \
--header "Accept: application/json" \
--write-out " Request complete. HTTP Status: %{http_code}\n" | jq .
\`\`\`

### Interpreting the Results

-   **`HTTP Status: 200`**: Success! The endpoint exists and your request was valid.
-   **`HTTP Status: 401/403`**: Unauthorized/Forbidden. Your token or credentials are likely invalid.
-   **`HTTP Status: 404`**: Not Found. The URL path is incorrect.
-   **`HTTP Status: 400`**: Bad Request. Check the JSON response for an error message; you may have a malformed parameter.

```

---

## File: docs/guides/managing-github-pat-authentication.md
```markdown
---
id: "guide:managing-github-pat-authentication"
fileExtension: "md"
title: "Guide: Managing GitHub PAT Authentication"
artifactVersion: "1.0.0"
status: "Active"
summary: "The official guide for generating and using a GitHub Personal Access Token (PAT) for secure command-line Git operations, as required by project policy."
usageGuidance:
  - "Follow these steps to create a PAT for authenticating with GitHub."
  - "Use this guide to troubleshoot common PAT authentication errors."
  - "This is the standard procedure for all developers needing CLI access to GitHub repositories."
owner: "Guardian, Scribe"
createdDate: "2025-06-27T00:00:00Z"
lastModifiedDate: "2025-06-27T00:00:00Z"
tags:
  - "guide"
  - "security"
  - "git"
  - "github"
  - "authentication"
  - "pat"
  - "cli"
  - "onboarding"
---
# Guide: Managing GitHub PAT Authentication

## 1. Purpose & Policy

This guide provides the standard procedure for authenticating with GitHub for command-line operations.

**Official Policy:** To ensure the security and integrity of our source code, all personnel must use a **Personal Access Token (PAT)** for any authenticated Git operation performed over HTTPS from the command line (e.g., `git push`, `git pull`). Using your account password for command-line operations is not permitted. This standard enforces best practices for accessing company-owned code.

## 2. Prerequisites

*   A GitHub account with access to the project's repositories.
*   A local machine with Git installed.
*   A locally cloned repository.

## 3. Procedure: Generating and Using a PAT

### 3.1. Generating Your PAT

1.  Log in to your GitHub account.
2.  Navigate to **Settings** > **Developer settings** > **Personal access tokens** > **Tokens (classic)**.
3.  Click **Generate new token** and select **Generate new token (classic)**.
4.  In the "Note" field, provide a descriptive name for the token (e.g., "Work-Laptop-YYYY-MM-DD").
5.  Select an **Expiration** for the token (e.g., 30 or 90 days).
6.  Select the `repo` scope. This is essential to grant access to repositories.
7.  Click **Generate token**.
8.  **Crucial:** Copy the generated token immediately and store it in a secure location (like a password manager). **You will not be able to view the token again after leaving this page.**

### 3.2. Authenticating from the Command Line

When you perform an action that requires authentication (like `git push`), you will be prompted for your credentials:

*   **Username:** Enter your GitHub username.
*   **Password:** Paste your **Personal Access Token (PAT)** here. Do not enter your GitHub password.

### 3.3. Caching Your Token with a Credential Helper (Recommended)

To avoid re-entering your PAT for every operation, you can configure Git to securely store it using a credential helper.

**On macOS (using osxkeychain):**
```bash
git config --global credential.helper osxkeychain
```

**On Windows (using Git Credential Manager):**
*(This is typically enabled by default with Git for Windows.)*
```bash
git config --global credential.helper manager
```

**On Linux (using a plain-text store for simplicity, requires configuration for secure storage):**
```bash
git config --global credential.helper store
```

The next time you authenticate, Git will store your PAT, and subsequent operations will not require a password entry.

## 4. Troubleshooting

*   **"Authentication failed" error:**
    *   Verify that your PAT has not expired.
    *   Confirm that the token has the correct `repo` scope.
    *   Ensure you are using your GitHub username, not your email address, at the username prompt.
*   **"Permission denied" error:**
    *   Confirm that your GitHub account has the necessary permissions (e.g., Write access) for the repository you are trying to push to.

```

---

## File: docs/guides/functional-options-pattern.md
```markdown
# Guide: The Functional Options Pattern

## 1. Purpose

This document provides the definitive explanation and implementation guide for the **Functional Options Pattern** in Go. This pattern is the project's standard for creating constructors for complex objects that have multiple optional configuration parameters. Its primary goal is to produce APIs that are readable, scalable, and backward-compatible.

## 2. The Core Pattern

The pattern involves four key components that work together.

### Step 1: Define the Target Struct with Unexported Fields

The process begins with the object that needs configuration. Its fields should be unexported to enforce encapsulation, forcing all initialization to go through the controlled constructor.

```go
// Server represents a server with configurable parameters.
// Its fields are unexported to enforce construction via NewServer.
type Server struct {
	host    string
	port    int
	timeout time.Duration
}
```

### Step 2: Define the `Option` Function Type

A function type, conventionally named `Option`, is defined. It is a function that accepts a pointer to the target struct.

```go
// Option is a function that configures a Server.
type Option func(*Server)
```

### Step 3: Create `With...` Option Constructors

For each configurable parameter, a public helper function is created, prefixed with `With`. These functions are closures that return a function of type `Option`.

```go
// WithHost sets the host for the Server.
func WithHost(host string) Option {
	return func(s *Server) {
		s.host = host
	}
}

// WithTimeout sets the timeout for the Server.
func WithTimeout(timeout time.Duration) Option {
	return func(s *Server) {
		s.timeout = timeout
	}
}
```

### Step 4: Implement the Variadic Constructor

The public constructor function is variadic, accepting zero or more `Option` arguments. It performs two critical steps:
1.  Initializes the struct with **sensible default values**.
2.  Iterates over the provided options, applying the user's customizations over the defaults.

```go
// NewServer creates and returns a new Server, configured with the provided options.
func NewServer(opts ...Option) *Server {
	// 1. Start with a server configured with default values.
	srv := &Server{
		host:    "localhost",
		port:    8080,
		timeout: 30 * time.Second,
	}

	// 2. Apply all the user-provided options.
	for _, opt := range opts {
		opt(srv)
	}

	return srv
}
```

## 3. Best Practice: Handling Required Parameters

Parameters that are **required** for the object to function **MUST** be explicit arguments in the constructor's signature. They must not be hidden inside an option.

```go
// CORRECT: The required 'port' is a direct parameter.
func NewServer(port int, opts ...Option) (*Server, error) {
    if port <= 0 {
        return nil, errors.New("port must be positive")
    }

	srv := &Server{
		port: port,
		// ...other defaults
	}

	for _, opt := range opts {
		opt(srv)
	}
	return srv, nil
}
```

This ensures that the compiler can enforce the provision of essential dependencies, preventing the creation of an object in an invalid state. Our `easyflor.NewClient` follows this pattern correctly by requiring a `tokenSource`.
```

---

## File: docs/guides/test-data-factory-pattern.md
```markdown
# Guide: The Test Data Factory Pattern

## 1. Purpose

This document provides the standard for creating test data in this project. We use a **Test Data Factory** with the **Functional Options Pattern** to create test fixtures. This approach solves the problem of creating complex test objects in a way that is readable, reusable, and maintainable.

It avoids messy test setup blocks with manually constructed structs and makes the intent of each test case crystal clear.

## 2. The Core Pattern

The pattern involves three components: a factory function, an option function type, and `With...` option helpers.

### Step 1: Define the Factory Function

The factory function's job is to return a **complete, valid instance** of the object with **sensible default values**. Its name should be descriptive, conventionally `NewMock...` or `NewTest...`.

```go
// NewMockPaymentRequest is a factory for creating a valid PostPaymentsRequest for tests.
func NewMockPaymentRequest(opts ...PaymentRequestOption) *PostPaymentsRequest {
	// 1. Create the object with sensible defaults.
	// This represents the most common, "happy path" version of the object.
	req := &PostPaymentsRequest{
		CardAccountId: "1234567890123456",
		Amount:        100,
		Source:        "Web",
		Address:       NewMockAddress(), // Factories can be composed.
	}

	// 2. Apply any overrides provided by the test.
	for _, opt := range opts {
		opt(req)
	}
	return req
}
```

### Step 2: Define the `Option` Function Type

We define a function type that can modify the object being built.

```go
// PaymentRequestOption is a function that can modify a PostPaymentsRequest.
type PaymentRequestOption func(*PostPaymentsRequest)
```

### Step 3: Create `With...` Option Constructors

For each field that a test might need to override, we create a public `With...` function. This function returns a closure of the `Option` type.

```go
// WithAmount allows a test to override the default amount.
func WithAmount(amount int) PaymentRequestOption {
	return func(req *PostPaymentsRequest) {
		req.Amount = amount
	}
}

// WithSource allows a test to override the default source.
func WithSource(source string) PaymentRequestOption {
	return func(req *PostPaymentsRequest) {
		req.Source = source
	}
}
```

## 3. Usage in Tests

This pattern makes the test setup code extremely clean and expressive. The test only needs to specify what is **different** for its specific scenario, rather than building the entire object from scratch.

```go
func TestPaymentProcessor(t *testing.T) {
	t.Run("should reject payments with a zero amount", func(t *testing.T) {
		// ARRANGE: Create a payment request that is valid by default,
		// but override only the amount to be zero for this specific test.
		zeroAmountRequest := NewMockPaymentRequest(
			WithAmount(0),
		)

		// ACT
		err := processor.Process(zeroAmountRequest)

		// ASSERT
		require.Error(t, err)
	})

	t.Run("should apply fraud detection for API transactions", func(t *testing.T) {
		// ARRANGE: Create a request specifically from the API source.
		apiRequest := NewMockPaymentRequest(
			WithSource("API"),
		)

		// ... ACT & ASSERT
	})
}
```

## 4. Key Principles

-   **Always Return a Valid Object:** The factory function without any options must return a complete and valid object.
-   **Use Functional Options for Overrides:** This keeps the test code clean and readable.
-   **Compose Factories:** For nested objects (like `Address` in our example), create separate factories and compose them.
```

---

## File: docs/lessons-learned/2025-08-09-advanced-etl-patterns.md
```markdown
---
title: "Lesson: Advanced ETL Patterns for Performance and Flexibility"
date: "2025-08-09"
status: "Active"
tags: ["lesson", "architecture", "refactoring", "etl", "parquet", "gocloud", "schema"]
---

### 1. Context

Following our initial architectural review, a second analysis of the `dui-go-ws-directory` repository was conducted. This analysis revealed more advanced and superior patterns for the "Load" and "Schema Management" phases of our ETL pipeline.

This document captures these key findings and provides a self-contained blueprint for their implementation, including critical code examples. This supersedes the need to retain the source repository as a reference.

### 2. Analysis & High-Level Recommendation

The analyzed repository contains two significant architectural concepts that would dramatically improve the performance, flexibility, and testability of our EasyFlor SDK.

**Recommendation:** We should adopt both concepts, starting with the generic Parquet writer, as it provides the most immediate and impactful benefits.

---

### 3. Concept 1: The Generic, Cloud-Agnostic Parquet Writer

*   **Current State:** Our `writers/bigquery` package is a simple, single-threaded writer that uses the BigQuery streaming API. This approach is inefficient for bulk loads, expensive, and tightly coupled to BigQuery, making it impossible to unit test without a live connection.

*   **Proposed Improvement:** Replace the current writer with a high-performance, concurrent, cloud-agnostic writer that streams data to object storage (like GCS) in the efficient Apache Parquet format. This is the industry-standard best practice for bulk data ingestion into data warehouses.

#### Key Implementation Examples

**A. The Provider (Cloud Abstraction):**
The core of this pattern is using the `gocloud.dev/blob` library to abstract away the specific cloud provider. This makes the writer instantly compatible with GCS, S3, Azure Blob Storage, and in-memory filesystems for testing.

```go
// From: src/parquetwriter/provider.go

import (
	"context"
	"gocloud.dev/blob"
	// Blank imports register the drivers we need
	_ "gocloud.dev/blob/gcsblob"
	_ "gocloud.dev/blob/memblob" // For testing
)

// The Provider manages the connection to a generic blob storage bucket.
type Provider struct {
	bucket *blob.Bucket
}

// The constructor takes a URL string, which determines the backend.
// e.g., "gs://my-bucket", "s3://my-bucket", or "mem://".
func NewProvider(ctx context.Context, bucketURL string) (*Provider, error) {
	bucket, err := blob.OpenBucket(ctx, bucketURL)
	// ... error handling ...
	return &Provider{bucket: bucket}, nil
}
```

**B. The Concurrent Writer (Producer/Consumer Pattern):**
The writer uses a producer/consumer pattern to decouple the transformation of data from the I/O of writing it, improving performance.

```go
// From: src/parquetwriter/writer.go

// The internal writer struct manages channels and waitgroups.
type writer[T any] struct {
	// ... fields for schema, transformer, etc. ...
	recordChan    chan arrow.Record // Channel for transformed Arrow records
	producerWg    sync.WaitGroup    // To wait for all transformations to finish
	consumerWg    sync.WaitGroup    // To wait for the single writer to finish
	// ... other fields ...
}

// Write() adds a record to a local buffer and flushes it when full,
// spinning up a 'producer' goroutine. This is fast and non-blocking.
func (w *writer[T]) Write(record T) {
	w.recordBuffer = append(w.recordBuffer, record)
	if len(w.recordBuffer) >= w.batchSize {
		w.flushBuffer() // flushBuffer starts the producer goroutine
	}
}

// The 'producer' goroutine transforms a batch of Go structs into an Arrow Record
// and sends it to the consumer via the channel.
func (w *writer[T]) producer(records []T) {
	// ... transforms records to an arrow.Record ...
	select {
	case w.recordChan <- arrowRecord: // Sends to consumer
	case <-w.ctx.Done():
	}
}

// The 'consumer' goroutine is a single writer that receives Arrow Records from
// all producers and writes them sequentially to the Parquet file.
func (w *writer[T]) consumer() {
	for record := range w.recordChan {
		w.pqWriter.Write(record)
		record.Release()
	}
}
```

---

### 4. Concept 2: Dynamic, On-Demand Schema Generation

*   **Current State:** Our schema tool (`tools/easyflor-tools`) generates a static schema from a hardcoded Go struct. It is not flexible.

*   **Proposed Improvement:** Evolve our schema generation to be dynamic. A user should be able to request a specific subset of fields, and the tool should generate a valid BigQuery schema for only those fields.

#### Key Implementation Example

This pattern combines a central "data dictionary" with a generator function that filters based on a user's request.

```go
// From: bqschema/user.go & transformers/user_schema.go

// 1. A master schema defines every possible field and how to extract it.
var UserSchema = map[string]FieldDefinition{
	"id":       {APIPath: "id", Extractor: func(u *admin.User) any { return u.Id }, ...},
	"fullName": {APIPath: "name/fullName", Extractor: func(u *admin.User) any { ... }, ...},
    // ...
}

// 2. A request struct defines what the user wants.
type SchemaFieldRequest struct {
	Name      string
	SubFields []string // For nested records like 'addresses'
}

// 3. The generator function iterates through the user's request, looks up
// the definition in the master schema, and builds the BigQuery schema.
func Generate(requestedFields []SchemaFieldRequest) (bigquery.Schema, error) {
	bqSchema := make(bigquery.Schema, 0)
	dummyUser := &admin.User{ ... } // Used to infer data types via reflection

	for _, req := range requestedFields {
		def, ok := UserSchema[req.Name]
		if !ok { continue }

        // Use the extractor on a dummy object to get a sample value.
		val := def.Extractor(dummyUser)
        // Convert the Go type of the value to a BigQuery type.
		fieldSchema, err := goTypeToBigQueryType(req.Name, def.Description, val, req.SubFields)
		if err != nil { return nil, err }

		bqSchema = append(bqSchema, fieldSchema)
	}
	return bqSchema, nil
}
```
```

---

## File: docs/lessons-learned/2025-08-09-architectural-improvements-from-analysis.md
```markdown
---
title: "Lesson: Architectural Patterns for a More Robust SDK"
date: "2025-08-09"
status: "Active"
tags: ["lesson", "architecture", "refactoring", "etl", "generics", "schema"]
---

### 1. Context

During the development of the EasyFlor SDK, we analyzed a parallel project (`dui-go-ws-directory`) to identify potential architectural improvements. This analysis revealed several advanced patterns that are superior to our current implementation, particularly in the areas of data fetching, transformation, and schema management.

This document captures the key findings and outlines a strategic plan for adopting these patterns. **It has been enhanced with key code examples to be a self-contained guide, making the original source repository obsolete for our purposes.**

### 2. Analysis & High-Level Recommendation

The `dui-go-ws-directory` project demonstrates a more mature architectural pattern for ETL pipelines. It decouples the definition of data from its transformation and abstracts common logic into reusable, generic helpers.

**Recommendation:** We should adopt three core concepts from the analysis to refactor the SDK.

---

### 3. Concept 1: The Generic Pagination Helper

*   **Current State:** The pagination logic in `debtors.go` and `purchases.go` is nearly identical, violating the DRY principle.
*   **Proposed Improvement:** Create a generic `fetchPaginatedResource` function that handles all boilerplate pagination logic.

#### Key Implementation Example

The core of this pattern is a generic function that accepts a `listFunc` closure. This allows it to work with any paginated API endpoint.

```go
// From: generic_fetch.go

// This is the function signature for the closure that the generic helper will call.
// It is responsible for making the specific API call (e.g., for users or groups).
type listFunc[T any] func(pageToken string) ([]T, string, error)

// This is the generic helper. It contains all the looping, error handling,
// and channel management logic in one place.
func fetchPaginatedResource[T any](ctx context.Context, logger *slog.Logger, list listFunc[T]) (<-chan T, <-chan error) {
	out := make(chan T)
	errc := make(chan error, 1)

	go func() {
		defer close(out)
		defer close(errc)
		var pageToken string

		for {
			// It calls the provided closure to get the next page of results.
			resources, nextPageToken, err := list(pageToken)
			if err != nil {
				// Contains robust, reusable error handling.
				errc <- err
				return
			}

			// It streams the results back on the output channel.
			for _, resource := range resources {
				select {
				case out <- resource:
				case <-ctx.Done():
					errc <- ctx.Err()
					return
				}
			}

			// It handles the loop termination logic.
			pageToken = nextPageToken
			if pageToken == "" {
				return
			}
		}
	}()

	return out, errc
}
```

---

### 4. Concept 2: The Schema-Driven, Dynamic Transformer

*   **Current State:** Our transformers are hardcoded to produce a specific Go struct, which is inflexible.
*   **Proposed Improvement:** Adopt a schema-driven approach where a central data dictionary defines all possible fields, and the transformer dynamically builds a `map[string]any` based on a user's request.

#### Key Implementation Example

This pattern relies on a "data dictionary" map that defines how to get each piece of data.

```go
// From: transformers/user_schema.go

// FieldDefinition defines the "recipe" for a single field.
type FieldDefinition struct {
	// The specific field mask needed by the Google API.
	APIPath     string
	// A function that knows how to extract this specific value from the source object.
	Extractor   func(*admin.User) any
	// The field's description for the data warehouse.
	Description string
}

// UserSchema is the master registry of all possible fields.
var UserSchema = map[string]FieldDefinition{
	"id": {
		APIPath: "id",
		Extractor: func(u *admin.User) any { return u.Id },
		Description: "The unique identifier for the user.",
	},
	"fullName": {
		APIPath: "name/fullName",
		Extractor: func(u *admin.User) any {
			if u.Name != nil { return u.Name.FullName }
			return nil
		},
		Description: "The user's full name.",
	},
    // ... many more field definitions
}
```

---

### 5. Concept 3: The Decoupled Schema Generation Package

*   **Current State:** Our BigQuery schema logic is tightly coupled within our CLI tool.
*   **Proposed Improvement:** Move the core schema generation logic into its own internal package (`internal/bqschema`) to make it testable and reusable.

#### Key Implementation Example

The core logic involves iterating through a user's request and using the "data dictionary" from Concept 2 to build the schema dynamically.

```go
// From: bqschema/user.go

// This function takes a list of requested field names.
func Generate(requestedFields []SchemaFieldRequest) (bigquery.Schema, error) {
	// It initializes an empty schema.
	bqSchema := make(bigquery.Schema, 0, len(requestedFields))

    // It uses a "dummy" version of the source object to infer data types.
	dummyUser := &admin.User{ ... }

	// It iterates over the requested fields.
	for _, req := range requestedFields {
        // It looks up the field's definition in the master schema (from Concept 2).
		def, ok := transformers.UserSchema[req.Name]
		if !ok {
			continue // Skips fields that don't exist.
		}

        // It uses the extractor on the dummy object to get a sample value.
		val := def.Extractor(dummyUser)

        // It translates the Go type of the sample value into a BigQuery type.
		fieldSchema, err := goTypeToBigQueryType(snakeCaseName, def.Description, val, req.SubFields)
		if err != nil {
			return nil, err
		}
		bqSchema = append(bqSchema, fieldSchema)
	}

	return bqSchema, nil
}
```

---

### 6. Conclusion

With these key implementation patterns now documented internally, this lesson learned is a self-contained guide for the proposed refactoring. The external `dui-go-ws-directory` repository is no longer required as a primary source for this effort.
```

---

## File: docs/lessons-learned/2025-09-06-aspos-elt-pipeline-strategy.md
```markdown
---
title: "Lesson: Adopting ELT Best Practices for the ASPOS Pipeline"
date: "2025-09-06"
status: "Active"
tags: ["lesson", "architecture", "elt", "aspos", "bigquery", "research"]
---

### 1. Context

During the refactoring of the `aspos-sync` example, external research was conducted to define a production-grade ELT (Extract, Load, Transform) pipeline strategy for POS data. This document internalizes the key findings from that research and outlines how they have directly informed the design of our implementation.

The core recommendation from the research is to move from a simple ETL pattern (transforming in the Go application) to a more robust and scalable ELT pattern, where raw data is loaded first and transformed later within BigQuery. While our current SDK implementation still performs the transformation in Go, this research guides our future direction and justifies the immediate adoption of stateful, incremental loading.

### 2. Key Research Finding: Incremental Loading is Critical

The research emphasizes that a production pipeline must not fetch the entire dataset on every run. The recommended approach is to perform incremental loads based on a time window or a monotonically increasing key.

*   **Our Implementation:** We have adopted this principle by refactoring the `aspos.TransactionSource` to fetch data based on a `since_id` parameter, using the transaction `ID` as the incremental key. This state is managed by a Firestore document, ensuring that each pipeline run only processes new records.

### 3. Key Research Finding: Staging and Transformation (ELT)

The ideal ELT pattern involves loading raw, unmodified JSON into a "landing" table in BigQuery and then using scheduled SQL queries (dbt, Dataform) to clean, model, and transform that data into production analytics tables.

*   **Our Implementation:** Our current example represents a hybrid "ETL" approach for simplicity. It transforms the data in the Go application before loading. However, by keeping the transformation logic in a dedicated `aspos.Transformer`, we have laid the groundwork to easily switch to an ELT pattern in the future. A future iteration could have the transformer simply pass through the raw `map[string]any` to a `JSON` column in a landing table.

### 4. Key Research Finding: BigQuery Schema Design

The research recommends a denormalized schema, using partitioning and clustering for optimal query performance.

*   **Our Implementation:** The `aspos.TransactionLandingRow` struct serves as the single source of truth for our BigQuery schema. The application's logic to auto-create the table from this struct ensures the schema is always correct and version-controlled with the code. While we have not yet implemented partitioning, this schema-first approach makes it simple to add partitioning and clustering options to the table creation logic in the future.

### 5. Conclusion

This research has been invaluable in elevating the `aspos-sync` example from a simple demonstration to a template for a production-ready, incremental pipeline. By capturing these findings here, we provide the strategic context for the design decisions made in the code.

---
*This document is an adaptation of the "Developer Guide: Building a POS Data ELT Pipeline from ASPOS API to BigQuery" research paper.*

```

---

## File: docs/personas/bolt.md
```markdown
---
title: "Persona: Bolt"
date: 2025-06-28T06:09:07Z
lastmod: 2025-06-28T06:09:07Z
draft: false
type: "persona"
description: "The definition and operating protocol for the Bolt persona, the team's core Software Developer."
tags: ["persona", "bolt", "developer", "code"]
---
# Persona: Bolt

## 1. Core Identity
- **Role:** Software Developer
- **Core Objective:** To write clean, efficient, and maintainable code that meets the acceptance criteria of a given Product Backlog Item (PBI).

## 2. Key Responsibilities
- **Code Implementation:** Writes and refactors application code.
- **Unit Testing:** Creates and maintains unit tests to ensure code quality and prevent regressions.
- **Adherence to Standards:** Follows the coding style guides and architectural patterns established for the project.
- **Collaboration:** Works closely with other personas, especially QA-Bot, to ensure features are implemented correctly.

## 3. Engagement Triggers
Bolt should be invoked when the team needs to:
- Implement a new feature or fix a bug.
- Refactor existing code for better performance or readability.
- Create or update unit tests.

```

---

## File: docs/personas/guardian.md
```markdown
---
title: "Persona: Guardian"
date: "2025-06-28T12:00:00Z"
lastmod: "2025-06-28T12:00:00Z"
draft: false
type: "persona"
description: "The definition and operating protocol for the Guardian persona, the team's Security and Compliance expert."
tags: ["persona", "guardian", "security", "compliance"]
---
# Persona: Guardian

## 1. Core Identity
- **Role:** Security and Compliance Expert
- **Core Objective:** To ensure the project adheres to security best practices and license compliance.

## 2. Key Responsibilities
- **Security Analysis:** Reviews code and dependencies for potential vulnerabilities.
- **License Compliance:** Verifies that all third-party libraries have compatible licenses.
- **Best Practice Enforcement:** Ensures that security best practices are followed throughout the development lifecycle.
- **Dependency Management:** Manages the `deps-update` task to keep dependencies current.

## 3. Engagement Triggers
Guardian should be invoked when the team needs to:
- Analyze the project for security vulnerabilities.
- Review new dependencies before they are added.
- Update project dependencies.

```

---

## File: docs/personas/helms.md
```markdown
---
title: "Persona: Helms"
date: 2025-06-28T07:03:59Z
lastmod: 2025-06-28T07:03:59Z
draft: false
type: "persona"
description: "The definition and operating protocol for the Helms persona, the team's Scrum Master, synthesized from authoritative sources."
tags: ["persona", "helms", "scrum-master", "process", "servant-leader", "playbook-v2"]
---
# Persona: Helms

## 1. Core Identity: The Servant-Leader

The Helms persona embodies the principle of **servant-leadership**. The primary motivation is to serve the team's needs first, fostering an environment of trust, safety, and empowerment where the team can perform at its highest potential.

Success is not measured by personal authority, but by the growth and increasing autonomy of the team.

## 2. Core Accountabilities (The Official Mandate)

As defined in the Scrum Guide, Helms is accountable for:
1.  **Establishing Scrum:** Ensuring the framework's theory, practices, and values are understood and enacted by both the team and the wider organization.
2.  **The Scrum Teams Effectiveness:** Enabling the team to improve its practices and deliver high-value Increments within the Scrum framework.

## 3. The Structure of Service

Helms's responsibilities are structured into three distinct areas:

### 3.1. Service to the Scrum Team
- **Coaching:** Coaches the team in self-management, cross-functionality, and ownership.
- **Focusing:** Helps the team focus on creating high-value Increments that meet the Definition of Done.
- **Removing Impediments:** Causes the removal of impediments to the team's progress, shielding them from external distractions.
- **Facilitating Events:** Ensures all Scrum events are positive, productive, and kept within their timebox.

### 3.2. Service to the Product Owner
- **Goal Definition:** Helps find effective techniques for defining the Product Goal and managing the Product Backlog.
- **Clarity & Transparency:** Helps the team understand the need for a clear and concise Product Backlog.
- **Stakeholder Facilitation:** Facilitates stakeholder collaboration as requested or needed to shorten feedback loops.

### 3.3. Service to the Organization
- **Adoption Leadership:** Leads, trains, and coaches the organization in its Scrum adoption.
- **Systemic Impediment Removal:** Works to remove barriers between stakeholders and Scrum Teams at a systemic level.
- **Fostering an Agile Ecosystem:** Plans and advises on Scrum implementations to increase the overall effectiveness of Scrum across the organization.

```

---

## File: docs/personas/logos.md
```markdown
---
title: "Persona: Logos"
date: "2025-06-28T12:00:00Z"
lastmod: "2025-06-28T12:00:00Z"
draft: false
type: "persona"
description: "The definition and operating protocol for the Logos persona, the team's AI Documentation Architect and Best Practices Researcher."
tags: ["persona", "logos", "research", "standards", "documentation-architecture"]
---
# Persona: Logos

## 1. Core Value Proposition
Logos researches and defines the optimal structures and principles for technical and AI-guidance documentation, ensuring our knowledge is captured in a reusable and effective way.

## 2. Primary Objectives
- To research and establish best practices for technical and AI-interaction documentation.
- To develop and provide structural templates for all project artifacts.
- To provide a foundational framework for how documentation and standards are created and maintained.

## 3. Key Competencies & Areas of Deep Expertise
- **Documentation Architecture:** Designing the structure of knowledge bases.
- **Best Practice Research:** Identifying industry-leading standards for documentation and AI prompting.
- **Template and Schema Design:** Creating reusable structures for documents and artifacts.
- **Tool-Assisted Research:** Proficient in using `concise_search` for quick lookups and `deep_research` for in-depth analysis.

## 4. Standard Research Protocol
To ensure the appropriate level of rigor is applied to every research task, Logos **MUST** adhere to the following protocol:

1.  **Assess Research Depth:** First, determine if the task requires a quick surface-level answer or a comprehensive, in-depth analysis.
2.  **Select the Correct Tool:**
    *   For **quick, factual lookups** (e.g., "What is the syntax for X?"), use the `concise_search` tool.
    *   For **foundational, strategic, or complex topics** (e.g., "What are the best practices for Y?"), use the `deep_research` tool.
3.  **Mandatory Research Brief for Deep Research:** Before executing `deep_research`, Logos **MUST** first prepare a formal "Research Brief" that outlines the objective, key research questions, and scope. This brief must be presented before the research is initiated.

## 5. Triggers for Engagement / When to Include This Persona
- **Include Logos when:**
  - A new type of documentation or artifact needs to be created.
  - The team needs to research the best way to solve a documentation or process problem.
  - Existing documentation standards need to be reviewed or updated.
  - A formal research brief is required for a complex topic.

## 6. Expected Contributions & Key Deliverables
- Structural templates for project artifacts.
- Research summaries on best practices.
- Foundational frameworks and schemas for documentation.

## 7. Primary Questions This Persona Helps Answer
- "What is the best way to structure this document?"
- "What are the industry standards for this type of process?"
- "How can we create a reusable template for this artifact?"

```

---

## File: docs/personas/qa-bot.md
```markdown
---
title: "Persona: QA-Bot"
date: "2025-06-28T12:00:00Z"
lastmod: "2025-06-28T12:00:00Z"
draft: false
type: "persona"
description: "The definition and operating protocol for the QA-Bot persona, the team's Quality Assurance specialist."
tags: ["persona", "qa-bot", "testing", "quality-assurance"]
---
# Persona: QA-Bot

## 1. Core Identity
- **Role:** Quality Assurance Specialist
- **Core Objective:** To verify that all changes meet the acceptance criteria and do not introduce new defects.

## 2. Key Responsibilities
- **Test Execution:** Runs the full suite of automated tests (`task test`).
- **Verification:** Analyzes code and documentation changes to confirm they align with the PBI's requirements.
- **Bug Reporting:** Clearly documents any bugs or discrepancies found during testing.
- **Acceptance Criteria Guardian:** Acts as the final gatekeeper to confirm that all acceptance criteria for a PBI have been met.

## 3. Engagement Triggers
QA-Bot should be invoked when the team needs to:
- Run the automated test suite.
- Perform a final verification of a feature before it is merged.
- Generate a verification context report (`task context verify`).

```

---

## File: docs/personas/scribe.md
```markdown
---
title: "Persona: Scribe"
date: 2025-06-28T06:10:35Z
lastmod: 2025-06-28T06:10:35Z
draft: false
type: "persona"
description: "The definition and operating protocol for the Scribe persona, the team's Technical Writer."
tags: ["persona", "scribe", "documentation", "writer"]
---
# Persona: Scribe

## 1. Core Identity
- **Role:** Technical Writer
- **Core Objective:** To create and maintain clear, comprehensive, and user-friendly documentation for the project.

## 2. Key Responsibilities
- **Documentation Creation:** Writes and updates READMEs, guides, playbooks, and other project documentation.
- **Glossary Maintenance:** Owns the project glossary, ensuring it is accurate and up-to-date.
- **Clarity and Consistency:** Reviews all documentation for clarity, consistency, and adherence to project standards.
- **Context Generation:** Helps generate context for documentation-heavy tasks.

## 3. Engagement Triggers
Scribe should be invoked when the team needs to:
- Create new documentation for a feature or process.
- Update existing documentation to reflect changes.
- Review a pull request for documentation quality.
- Generate context for a pull request (`task context pr`).

```

---


# Iteration 11: Framework Meta-Data Removal
The following files were removed as they describe the internal processes of the template framework, not the project.

## File: docs/glossary.md
```markdown
---
title: "Project Glossary"
date: "2025-06-28T12:00:00Z"
lastmod: "2025-06-28T12:00:00Z"
draft: false
type: "glossary"
description: "The single source of truth for all project-specific terminology, acronyms, and concepts."
tags: ["glossary", "definitions", "onboarding", "standards"]
---
# Project Glossary

This document defines the key terms used throughout this project and its automation framework.

## F

### Factory
The automation framework for the project, located in the `/factory` directory. It contains all the commands used to build, test, analyze, and deploy the product.

## L

### Library
The collection of project-specific documentation, including guides, standards, and playbooks. It is located in the `/docs` directory and serves as the single source of truth for how to work on the project.

## O

### Orchestrator
The human lead of the team. The Orchestrator holds two stances: the **Product Owner Stance** (focused on the "what" and "why") and the **Lead Developer Stance** (focused on the "how" and the execution of tasks).

## P

### Persona
A specialized AI assistant designed to embody a specific role and set of competencies within the Scrum team (e.g., Helms, Scribe, Logos).

### PBI (Product Backlog Item)
A single unit of work in the product backlog. It represents a feature, bug fix, documentation update, or other task that delivers value.

### Product
The deployable application or library. In this project, the product is the **`template-flow-sdk` library** located in the project root.

```

---

## File: docs/guides/style-guide.md
```markdown
---
id: "guide:style-guide"
fileExtension: "md"
title: "THEA Contributor Style Guide"
artifactVersion: "1.2.0" # Version bump for metadata update
status: "Active"
summary: "The definitive guide defining content standards for the THEA framework, including filename conventions, artifact status lifecycle, and other stylistic rules."
usageGuidance:
  - "Consult this guide before creating or modifying any new files or documentation to ensure consistency."
  - "Primary reference for the 'lowercase-kebab-case' filename convention."
owner: "Canon"
createdDate: "2025-06-07T12:00:00Z"
lastModifiedDate: "2025-06-13T05:02:00Z" # Reflects current update time
tags:
  - "style-guide"
  - "standards"
  - "documentation"
  - "contribution"
  - "process"
  - "naming-convention"
  - "lifecycle"
---
# THEA Contributor Style Guide

**Version:** 1.1
**Status:** Active
**Conceptual Owner:** `Canon`

## 1. Purpose

This document provides essential style and formatting guidelines for all contributors to the THEA framework. Adhering to these standards ensures consistency, clarity, and maintainability across the entire project, making our artifacts easier to read, navigate, and manage.

## 2. Artifact Metadata (Front Matter)

All formal artifacts (documents, playbooks, schemas) MUST begin with a YAML front matter block that conforms to the `thea_artifact_metadata_schema.json`.

### 2.1. Artifact Status

The `status` field is mandatory and defines the artifact's position in its lifecycle. Use one of the following values:

- **`Draft`**: A work-in-progress, not yet ready for general use or review.
- **`Proposed`**: Ready for review by the team. The content is complete but not yet officially approved.
- **`Active`**: The official, current version of the artifact. It should be used and adhered to.
- **`Deprecated`**: The artifact is outdated and scheduled for removal. It should be avoided, and its replacement should be referenced.
- **`Archived`**: No longer in use but kept for historical context. It should not be deleted.

## 3. Filename Conventions

To ensure cross-platform compatibility, prevent linking issues, and reduce cognitive load, all files created within this project must adhere to a standardized naming convention.

### 3.1. The Standard: `lowercase-kebab-case`

All new files and directories MUST use **lowercase-kebab-case**.

- **Description:** This means all letters are lowercase, and words are separated by hyphens (`-`).
- **Correct Examples:**
  - `docs/guides/new-user-guide.md`
  - `thea/schemas/artifact-metadata-schema.json`
  - `playbooks/process-guidance/managing-project-diagrams.md`
- **Incorrect Examples:**
  - `NewUserGuide.md` (PascalCase)
  - `newUserGuide.md` (camelCase)
  - `new_user_guide.md` (snake_case)
  - `New user guide.md` (contains spaces)

### 3.2. Exceptions

A few specific, universally recognized root-level files SHOULD retain their conventional casing. This is because they are often treated specially by Git hosting platforms, build tools, or community expectations.

**The only exceptions are:**

- `README.md`
- `LICENSE`
- `CONTRIBUTING.MD` (or `CONTRIBUTING.md`)
- `CHANGELOG.MD` (or `CHANGELOG.md`)

All other files in all other directories MUST follow the `lowercase-kebab-case` standard.

### 3.3. Adoption Policy

A full, immediate renaming of all existing files is not required as it would create unnecessary noise in the Git history. Instead, we will follow a "going forward" policy:

1. **All new files** MUST be created using the `lowercase-kebab-case` standard.
2. **Existing files** that do not conform to the standard SHOULD be renamed **opportunistically** when they are next undergoing significant modification or review. Do not create a Pull Request solely for renaming files.

## 4. Markdown Linting and Quality

To automate the enforcement of Markdown standards, this project uses `markdownlint`.

### 4.1. Development Environment Integration

To provide immediate feedback, the `markdownlint-cli` tool is integrated into the Firebase Studio development environment via the project's `.idx/dev.nix` file. This is managed by `Sparky`.

The `davidanson.vscode-markdownlint` VS Code extension is also a recommended part of the workspace configuration to provide real-time, in-editor highlighting of any issues.

### 4.2. Contributor Responsibility

All contributors are expected to resolve any `markdownlint` errors reported in the editor before submitting a Pull Request. This ensures that all committed documentation adheres to our shared quality standards from the beginning.

```

---

## File: docs/playbooks/knowledge-filing-playbook.md
```markdown
# Playbook: Knowledge Filing Protocol

- **Objective:** To provide a standard process for documenting new knowledge (e.g., guides, lessons, playbooks) and determining its correct "filing cabinet."
- **Philosophy:** This protocol embodies the "Start Small, Learn Fast" principle. All knowledge is born from the context of a specific project and is only replicated to the core framework after it has proven its value.

---

## Process

This playbook outlines the standard procedure for filing any new piece of documentation.

| Step | Action | Details / Rationale |
| :--- | :--- | :--- |
| **1** | **Identify New Knowledge** | A new lesson is learned, a process is defined, or a standard is created during project work. |
| **2** | **File Locally (Mandatory)** | Create a new Markdown document and file it within the **current project's** `docs/` directory. <br><br> *This is the default and mandatory first step. It ensures immediate value is captured and available to the project team.* |
| **3** | **Evaluate for Generalization** | After the document is created, perform the **Litmus Test**: <br><br> *"Would every new project started from this template benefit from this document?"* |
| **4** | **Replicate to THEA (Conditional)** | **If YES:** A separate task should be initiated to adapt, generalize, and contribute the document to the canonical **THEA Framework** repository. This is a "replication" of a proven idea, not a "promotion." <br><br> **If NO:** The process is complete. The knowledge correctly remains specific to this project. |


```

---

